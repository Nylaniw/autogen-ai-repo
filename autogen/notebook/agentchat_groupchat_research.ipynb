{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Performs Research with Multi-Agent Group Chat\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install \"pyautogen>=0.2.3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well).\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k-0314',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb) for full code examples of the different methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 01-19 08:07:29] {94} WARNING - openai client was provided with an empty config_list, which may not be intended.\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m gpt4_config \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcache_seed\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m42\u001b[39m,  \u001b[39m# change the cache_seed for different trials\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconfig_list\u001b[39m\u001b[39m\"\u001b[39m: config_list_gpt4,\n\u001b[0;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m120\u001b[39m,\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      7\u001b[0m user_proxy \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mUserProxyAgent(\n\u001b[0;32m      8\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAdmin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     system_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mA human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     code_execution_config\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m engineer \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39;49mAssistantAgent(\n\u001b[0;32m     13\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEngineer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m     llm_config\u001b[39m=\u001b[39;49mgpt4_config,\n\u001b[0;32m     15\u001b[0m     system_message\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39mEngineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt modify your code. So do not suggest incomplete code which requires others to modify. Don\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt use a code block if it\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms not intended to be executed by the executor.\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[39mDon\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[39mIf the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m scientist \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mAssistantAgent(\n\u001b[0;32m     21\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScientist\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     llm_config\u001b[39m=\u001b[39mgpt4_config,\n\u001b[0;32m     23\u001b[0m     system_message\u001b[39m=\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mScientist. You follow an approved plan. You are able to categorize papers after seeing their abstracts printed. You don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt write code.\u001b[39m\u001b[39m\"\"\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m planner \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mAssistantAgent(\n\u001b[0;32m     26\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlanner\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     system_message\u001b[39m=\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mPlanner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     llm_config\u001b[39m=\u001b[39mgpt4_config,\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\winalyn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogen\\agentchat\\assistant_agent.py:61\u001b[0m, in \u001b[0;36mAssistantAgent.__init__\u001b[1;34m(self, name, system_message, llm_config, is_termination_msg, max_consecutive_auto_reply, human_input_mode, code_execution_config, description, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     33\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     34\u001b[0m     name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     43\u001b[0m ):\n\u001b[0;32m     44\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m        name (str): agent name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m            [ConversableAgent](conversable_agent#__init__).\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         name,\n\u001b[0;32m     63\u001b[0m         system_message,\n\u001b[0;32m     64\u001b[0m         is_termination_msg,\n\u001b[0;32m     65\u001b[0m         max_consecutive_auto_reply,\n\u001b[0;32m     66\u001b[0m         human_input_mode,\n\u001b[0;32m     67\u001b[0m         code_execution_config\u001b[39m=\u001b[39mcode_execution_config,\n\u001b[0;32m     68\u001b[0m         llm_config\u001b[39m=\u001b[39mllm_config,\n\u001b[0;32m     69\u001b[0m         description\u001b[39m=\u001b[39mdescription,\n\u001b[0;32m     70\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     71\u001b[0m     )\n\u001b[0;32m     73\u001b[0m     \u001b[39m# Update the provided description if None, and we are using the default system_message,\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[39m# then use the default description.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m description \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\winalyn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:126\u001b[0m, in \u001b[0;36mConversableAgent.__init__\u001b[1;34m(self, name, system_message, is_termination_msg, max_consecutive_auto_reply, human_input_mode, function_map, code_execution_config, llm_config, default_auto_reply, description)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm_config, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    125\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_config\u001b[39m.\u001b[39mupdate(llm_config)\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m OpenAIWrapper(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_config)\n\u001b[0;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_code_execution_config: Union[Dict, Literal[\u001b[39mFalse\u001b[39;00m]] \u001b[39m=\u001b[39m (\n\u001b[0;32m    129\u001b[0m     {} \u001b[39mif\u001b[39;00m code_execution_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m code_execution_config\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuman_input_mode \u001b[39m=\u001b[39m human_input_mode\n",
      "File \u001b[1;32mc:\\Users\\winalyn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogen\\oai\\client.py:105\u001b[0m, in \u001b[0;36mOpenAIWrapper.__init__\u001b[1;34m(self, config_list, **base_config)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_list \u001b[39m=\u001b[39m [\n\u001b[0;32m    101\u001b[0m         {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopenai_kwargs}}\n\u001b[0;32m    102\u001b[0m         \u001b[39mfor\u001b[39;00m config \u001b[39min\u001b[39;00m config_list\n\u001b[0;32m    103\u001b[0m     ]\n\u001b[0;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clients \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client(extra_kwargs, openai_config)]\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_list \u001b[39m=\u001b[39m [extra_kwargs]\n",
      "File \u001b[1;32mc:\\Users\\winalyn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogen\\oai\\client.py:162\u001b[0m, in \u001b[0;36mOpenAIWrapper._client\u001b[1;34m(self, config, openai_config)\u001b[0m\n\u001b[0;32m    160\u001b[0m openai_config \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopenai_config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopenai_kwargs}}\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_for_azure(openai_config, config)\n\u001b[1;32m--> 162\u001b[0m client \u001b[39m=\u001b[39m OpenAI(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopenai_config)\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m client\n",
      "File \u001b[1;32mc:\\Users\\winalyn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_client.py:99\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m     97\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    100\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "gpt4_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.\n",
    "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
    "\"\"\",\n",
    ")\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Scientist. You follow an approved plan. You are able to categorize papers after seeing their abstracts printed. You don't write code.\"\"\",\n",
    ")\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"\"\"Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\n",
    "The plan may involve an engineer who can write code and a scientist who doesn't write code.\n",
    "Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a scientist.\n",
    "\"\"\",\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"paper\"},\n",
    ")\n",
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    system_message=\"Critic. Double check plan, claims, code from other agents and provide feedback. Check whether the plan includes adding verifiable info such as source URL.\",\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor, critic], messages=[], max_round=50\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'manager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m user_proxy\u001b[39m.\u001b[39minitiate_chat(\n\u001b[1;32m----> 2\u001b[0m     manager,\n\u001b[0;32m      3\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfind papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'manager' is not defined"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Group Chat without Critic for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape the arXiv website for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data to identify the different domains in which LLM is applied. This could be based on keywords in the title or abstract, or the scientist's knowledge of the field.\n",
      "\n",
      "3. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist. The script should output a markdown table with columns for the domain, title, authors, abstract, and link.\n",
      "\n",
      "4. Scientist: Review the markdown table to ensure the papers are correctly categorized and the information is accurate.\n",
      "\n",
      "5. Engineer: Make any necessary revisions to the script based on the scientist's feedback.\n",
      "\n",
      "6. Scientist: Give final approval of the markdown table.\n",
      "\n",
      "7. Engineer: Submit the final markdown table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "groupchat_nocritic = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor], messages=[], max_round=50\n",
    ")\n",
    "for agent in groupchat.agents:\n",
    "    agent.reset()\n",
    "manager_nocritic = autogen.GroupChatManager(groupchat=groupchat_nocritic, llm_config=gpt4_config)\n",
    "user_proxy.initiate_chat(\n",
    "    manager_nocritic,\n",
    "    message=\"\"\"\n",
    "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
    "\"\"\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
